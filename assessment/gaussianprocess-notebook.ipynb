{"cells":[{"cell_type":"markdown","metadata":{},"source":"Gaussian Processes - Submission Notebook\n========================================\n\n"},{"cell_type":"markdown","metadata":{},"source":["## Gaussian Processes\n\n"]},{"cell_type":"markdown","metadata":{},"source":["This notebook will form part of your individual submission for the course. The notebook will roughly mimick the parts that are in the `PDF` worksheet. Your task is to complete the code that is missing in the parts below and answer the questions that we ask. The aim is **not** for you to **solve** (I'm not actually sure what that means) the worksheet but rather for you to show your understanding of the material in the course, instead of re-running and aiming to get \"perfect\" results run things, make sure it is correct and then try to explain your results with a few sentences. Actually if it is not correct and you explain things well that might actually teach you more things.\n\nWe are going to work with GPs in exactly the same setting as in the case of linear regression. This means we are interested in the task of observing a data-set $\\mathcal{D} = \\{x_i, y_i \\}_{i=1}^N$ where we assume the following relationship between the variates,  \n$$\ny_i = f(x_i).\n$$\nOur task is to infer the function $f(\\cdot$) from $\\mathcal{D}$. We will make the same assumptions of independence as in the linear regression case which leads us to the following likelihood function,\n$$\np(\\mathbf{y}\\vert f(\\mathbf{x})) = \\prod_{i=1}^N p(y_i \\vert f(x_i)),\n$$\nwhere $f(x_i)$ is the output of the function at location $x_i$.\n\n**Question 1**\nExplain what the independence assumption implies? How would the likelihood look if we could not make this assumption?/\n\n**Answer**\n\nNow lets get our hands dirty and do some implementation so lets import some basic libraries that we are going to need for the lab. We will also set the random number seed.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import cdist"]},{"cell_type":"markdown","metadata":{},"source":["### Prior\n\n"]},{"cell_type":"markdown","metadata":{},"source":["We will now proceed to define the prior over the function values $p(\\mathbf{f})$, we will do so using a Gaussian process. The prior being non-parametric needs in addition to a set of parameters $\\boldsymbol{\\theta}$ also access to the location where we want to evaluate it $\\mathbf{x}$. This means the distribution that we are going to parametrise is,\n$$\np(\\mathbf{f}\\mid \\mathbf{x},\\boldsymbol{\\theta}) = \\mathcal{N}\\left(\\mathbf{f}\\mid \\mu(\\mathbf{x}, k(\\mathbf{x},\\mathbf{x}) \\right).\n$$\nWe will begin by implementing the co-variance function $k(x_i,x_j)$. We will play around with\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def rbf_kernel(x1, x2, varSigma, lengthscale):\n    if x2 is None:\n        d = cdist(x1, x1)\n    else:\n        d = cdist(x1, x2)\n        K = varSigma*np.exp(-np.power(d, 2)/lengthscale)\n    return K\n\ndef lin_kernel(x1, x2, varSigma):\n    if x2 is None:\n        return varSigma*x1.dot(x1.T)\n    else:\n        return varSigma*x1.dot(x2.T)\n    \ndef white_kernel(x1, x2, varSigma):\n    if x2 is None:\n        return varSigma*np.eye(x1.shape[0])\n    else:\n        return np.zeros(x1.shape[0], x2.shape[0])\n    \ndef periodic_kernel(x1, x2, varSigma, period, lengthScale):\n    if x2 is None:\n        d = cdist(x1, x1)\n    else:\n        d = cdist(x1, x2)\n    return varSigma*np.exp(-(2*np.sin((np.pi/period)*d)**2)/lengthScale**2)"]},{"cell_type":"markdown","metadata":{},"source":["**Question 2** Evaluate the four covariance functions implemented above for a one dimensional index set, say `x = np.linspace(-6,6,100).reshape(-1,1)` and plot the result using `plt.imshow()`. Play around with the input parameters to the co-variance functions to make sure you have an idea of what they do and how the alter the matrix. Explain briefly why the matrices look the way they do and what it will imply for which functions we find likely.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["x = np.linspace(-6,6,100).reshape(-1,1)"]},{"cell_type":"markdown","metadata":{},"source":["**Answer**\n\nNow when we have implemented the co-variance function we can generate samples from the prior. We can use the function `np.random.multivariate_normal()` in order to sample points from a multi-variate Gaussian. We will use a zero-mean process so the mean function is just constant $0$.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["x = np.linspace(-6,6,200).reshape(-1,1)\nK = rbf_kernel(x,x,1,1)\nmu = np.zeros(x.shape)\n\nf = np.random.multivariate_normal(mu.flatten(), K, 20)\nfig = plt.figure(figsize=(10,5))\nax = fig.add_subplot(111)\nax.plot(x, f.T)"]},{"cell_type":"markdown","metadata":{},"source":["Play around a little bit with the parameters of the co-variance function and generate samples. Try to build up a notion of how the samples are controlled by the parametrisation. Make sure that you can get the connection between the co-variance matrix that we visualised and the samples that you see.\n\n**Question 3** So far we have used a mean function that is constant zero. Try to use a different type of mean-function, first try with a linear-linear function (i.e. a line) and then try a periodic function like a sine. Explain the effect that the mean function have on the samples that you draw.n\n\n**Answer**\n\nOne interesting thing about co-variance functions are that they are closed under lots of operations. This means that you can combine them in different ways. Try to generate samples from a process where your prior is the `sum` the `product` of several of these functions. Experiment with this a bit, here is where you can input a lot of knowledge into the system.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Posterior\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Now when we have seen samples from the prior it is time to merge these assumptions with observed data. The first thing we will do is to generate some data in a way that we allow us to test things. We will generate two data-sets, a one-dimensional and a two-dimensional function.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def data_1d(N, noise):\n    x = np.random.randn(N)\n    x = np.sort(x)\n    y = -(-np.sin(3*x) - x**2 + 0.7*x) + noise*np.random.randn(x.shape[0])\n\n    return x,y\n\n\ndef data_2d(N, noise):\n    xg = np.linspace(-4,4,N)\n    x1, x2 = np.meshgrid(xg,xg)\n    x1 = x1.flatten()\n    x2 = x2.flatten()\n\n    y = (x1**2+x2-11)**2 + (x1+x2**2-7)**2\n\n    X = np.stack((x1,x2)).T\n    \n    return X, y"]},{"cell_type":"markdown","metadata":{},"source":["Now when we have some data we will get our updated belief about the function values given the prior assumptions and the data we have observed. To do so we need to formulate the predictive distribution of the Gaussian process.\n\n**Question 4** Explain how we reach the updated belief about our function given the data? What is the specific process that we have to go through? Explain this both for any general model and what is unique (rather unique actually) about how we do this using a Gaussian process prior?\n\n**Answer**\n\n**Question 5** Now look first at the formulation of the predictive posterior. We can see that the posterior variance consists of two terms, one substracted from the other. Explain what the effect of these two terms are and try to provide an intuition of how it leads to the posterior variance. One good way to do this is to plot each of the terms individually. \n\nNow let us implement the predictive posterior evaluated at data `x1, y1` and evaluated at `xstar`. You might want to be a bit more clever and implement a generic function that takes the matrices as input instead so that you can try different co-variance functions.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def gp_prediction(x1, y1, xstar, lengthScale, varSigma, noise):\n\n\tk_starX = rbf_kernel(xstar,x1,lengthScale,varSigma,noise)\n\tk_xx = rbf_kernel(x1, None, lengthScale, varSigma, noise)\n\tk_starstar = rbf_kernel(xstar,None,lengthScale,varSigma,noise)\n\n\tmu = k_starX.dot(np.linalg.inv(k_xx)).dot(y1)\n\tvar = k_starstar - (k_starX).dot(np.linalg.inv(k_xx)).dot(k_starX.T)\n\n\treturn mu, var, xstar"]},{"cell_type":"markdown","metadata":{},"source":["**Question 6** Using the function above, generate samples using the one-dimensional data. Then generate a plot where you show the mean and the marginal variance at each function evaluation. You can generate the plots that you have seen in the lectures using `np.fill_between()`.\n\n**Answer**\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["#generate the plots"]},{"cell_type":"markdown","metadata":{},"source":["In the final step covering the material that we have covered in the lectures we are now going to include the likelihood function that we first specified and write down the predictive posterior. This alters the posterior to the one below if we have a likelihood with precision $\\beta$.\n\n\\begin{align}\np(\\mathbf{f}_*\\vert \\mathbf{y}, \\mathbf{x},\\mathbf{x}_*,\\theta) &= \\mathcal{N}(\\mu_{\\mathbf{x}_*\\vert \\mathbf{x}}, K_{\\mathbf{x}_*\\vert \\mathbf{x}})\\\\\n\\mu_{\\mathbf{x}_*\\vert \\mathbf{x}} &= k(\\mathbf{x}_*,\\mathbf{x})\\left(k(\\mathbf{x},\\mathbf{x})+\\frac{1}{\\beta}\\mathbf{I}\\right)^{-1}\\mathbf{y}\\\\\nK_{\\mathbf{x}_*\\vert \\mathbf{x}} &= k(\\mathbf{x}_*,\\mathbf{x}_*) - k(\\mathbf{x}_*,\\mathbf{x})\\left(k(\\mathbf{x},\\mathbf{x})+\\frac{1}{\\beta}\\mathbf{I}\\right)^{-1}k(\\mathbf{x},\\mathbf{x}_*).\n\\end{align}\n\n**Question 7** Explain why the predictive posterior changes in the way it does? How does the samples of the function change from when we did not include this term? Think about the concept of \"explaining away\" that we discussed during the lectures.\n\n**Answer**\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Learning\n\n"]},{"cell_type":"markdown","metadata":{},"source":["In this final part we will go a bit beyond the material that we have covered in the lectures. So far we have specified the Gaussian process prior, performed the marginalisation and computed the predictive posterior. In this case the hyper-parameters, the parameters of the co-variance function have been fixed. We will now proceed to optimise the marginal likelihood as a way of \"learning\" the parameters. This means that our learning task is to,\n$$\n\\hat{\\boldsymbol{\\theta}} = \\operatorname{argmax}_{\\boldsymbol{\\theta}} p(\\mathbf{y}\\mid\\mathbf{X},\\boldsymbol{\\theta}).\n$$\nNow in order to make things a little bit nicer for us to optimise we are instead of looking at the likelihood going to use the negative log-marginal likelihood instead and try to find the parameters that minimises this,\n$$\n\\hat{\\boldsymbol{\\theta}} = \\operatorname{argmin}_{\\boldsymbol{\\theta}} - \\textrm{log}\\left( p(\\mathbf{y}\\mid\\mathbf{X},\\boldsymbol{\\theta})\\right) = \\mathcal{L}(\\boldsymbol{\\theta})\n$$\n\n**Question 8** Explain why these two objective functions lead to the same optimisation problem?\n\n**Answer**\n\nNow lets write up the log-marginal likelihood,\n$$\n\\operatorname{log} p(\\mathbf{y}\\mid \\mathbf{X}, \\boldsymbol{\\theta}) = \\int p(\\mathbf{y}\\mid \\mathbf{f})p(\\mathbf{f}\\mid \\mathbf{X},\\boldsymbol{\\theta})\\textrm{d}\\boldsymbol{\\theta}\n$$\n$$\n-\\frac{1}{2}\\mathbf{y}^{\\textrm{T}}\\left(k(\\mathbf{x},\\mathbf{x})\\right)^{-1}\\mathbf{y} - \\frac{1}{2}\\operatorname{log}\\operatorname{det}\\left(k(\\mathbf{x},\\mathbf{x}-\\beta^{-1}\\mathbf{I}\\right) - \\frac{N}{2}\\operatorname{log}2\\pi = -\\mathcal{L}(\\boldsymbol{\\theta})\n$$\n\nWe will perform the optimisation using a gradient based optimiser. This means that we need to compute the gradients of the loss-function $\\mathcal{L}$ with respect to $\\boldsymbol{\\theta}$. While these gradients are not very hard to compute by hand, and you are more than free to do so, we can also use packages that can handle `auto-differentiation`. There are lots of them around but my preference is to use `jax` which nicely follows `numpy`. Below is some scaffolding to a simple gradient based optimisation loop.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from jax import grad\nimport jax.numpy as jnp\n\ndef squared_exponential(x1, x2, theta):\n    # theta[0] - variance\n    # theta[1] - lengthscale\n    # theta[2] - noise variance\n    if x2 == None:\n        return theta[0]*jnp.exp(-cdist(x1, x1, metric='sqeuclidean')/theta[1]**2) + (1/theta[2])*jnp.eye(x.shape[0])\n    else:\n        return theta[0]*jnp.exp(-cdist(x1, x2, metric='sqeuclidean')/theta[1]**2)\n\ndef logmarginal_likelihood(x, y, theta):\n    # implement the log-marginal likelihood of a GP\n    \ndLdtheta = grad(logmarginal_likelihood, argnums=3)\nfor i in range(1000):\n    \n    theta -= dLdtheta(theta) * 0.01"]},{"cell_type":"markdown","metadata":{},"source":["Finish the code above and try to learn the hyper-parameters given a set of observed data. You might want to use data that is quite noisy and where you do not have too many data-points to avoid some computational issues that might emerge. If you are finding that the optimisation somehow crashes, try to remove `theta[2]` from the optimisation and make it a fixed value.\n\n**Question 9** Show a plot where you show both the predictive posterior of the initial setting (the start values of $\\boldsymbol{\\theta}$ and the optimised values. Explain the results.\n\n**Answer**\n\n**Question 10** In the co-variance function `def squared_exponential` above we have two linear parameters `theta[0]` and `theta[2]` that scales the two terms of the function. Interpret these two parameters, what does the relation between the two mean? Hint: rather than thinking about precision think about `1/theta[2]` which is related to the variance.\n\n**Answer**\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Submission\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Now you are done with this notebook and hopefully you feel that you have a good understanding of Gaussian processes both in theory and in practice. The key thing when we move onto to the next parts of the course is that you start building up an intuition for what you can do with these models, how you can alter things, what assumptions/knowledge that are easy to encode etc. Please submit your notebook on Moodle before the deadline which is **Friday 20th of October 12:00**\n\n"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}
