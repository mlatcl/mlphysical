---
title: "Emulation and Experimental Design"
venue: "Virtual (Zoom)"
abstract: "<p>This lecture will review the ideas behind using surrogate models for experimental design.</p>"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: 
  twitter: lawrennd
  gscholar: r3SJcvoAAAAJ
  orchid: 
date: 2020-10-29
published: 2020-10-29
week: 4
reveal: 04-experimental-design.slides.html
ipynb: 04-experimental-design.ipynb
layout: lecture
categories:
- notes
---



<!-- Do not edit this file locally. -->
<!---->
<!-- Do not edit this file locally. -->
<!-- Do not edit this file locally. -->
<!-- The last names to be defined. Should be defined entirely in terms of macros from above-->
<!--

-->
<h1 id="emulation">Emulation</h1>
<h2 id="statistical-emulation">Statistical Emulation</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_uq/includes/emulation.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_uq/includes/emulation.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="figure">
<div id="statistical-emulation-1-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/statistical-emulation000.svg" width="80%" style=" ">
</object>
</div>
<div id="statistical-emulation-1-magnify" class="magnify" onclick="magnifyFigure(&#39;statistical-emulation-1&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="statistical-emulation-1-caption" class="caption-frame">
<p>Figure: Real world systems consiste of simulators, that capture our domain knowledge about how our systems operate. Different simulators run at different speeds and granularities.</p>
</div>
</div>
<p>In many real world systems, decisions are made through simulating the environment. Simulations may operate at different granularities. For example, simulations are used in weather forecasts and climate forecasts. The UK Met office uses the same code for both, but operates climate simulations one at greater spatial and temporal resolutions.</p>
<div class="figure">
<div id="statistical-emulation-2-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/statistical-emulation001.svg" width="80%" style=" ">
</object>
</div>
<div id="statistical-emulation-2-magnify" class="magnify" onclick="magnifyFigure(&#39;statistical-emulation-2&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="statistical-emulation-2-caption" class="caption-frame">
<p>Figure: A statistical emulator is a system that reconstructs the simulation with a statistical model.</p>
</div>
</div>
<p>A statistical emulator is a data-driven model that learns about the underlying simulation. Importantly, learns with uncertainty, so it ‚Äòknows what it doesn‚Äôt know‚Äô. In practice, we can call the emulator in place of the simulator. If the emulator ‚Äòdoesn‚Äôt know‚Äô, it can call the simulator for the answer.</p>
<div class="figure">
<div id="statistical-emulation-5-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/statistical-emulation004.svg" width="80%" style=" ">
</object>
</div>
<div id="statistical-emulation-5-magnify" class="magnify" onclick="magnifyFigure(&#39;statistical-emulation-5&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="statistical-emulation-5-caption" class="caption-frame">
<p>Figure: A statistical emulator is a system that reconstructs the simulation with a statistical model. As well as reconstructing the simulation, a statistical emulator can be used to correlate with the real world.</p>
</div>
</div>
<div class="figure">
<div id="statistical-emulation-6-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/statistical-emulation005.svg" width="80%" style=" ">
</object>
</div>
<div id="statistical-emulation-6-magnify" class="magnify" onclick="magnifyFigure(&#39;statistical-emulation-6&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="statistical-emulation-6-caption" class="caption-frame">
<p>Figure: In modern machine learning system design, the emulator may also consider the output of ML models (for monitoring bias or accuracy) and Operations Research models..</p>
</div>
</div>
<p>As well as reconstructing an individual simulator, the emulator can calibrate the simulation to the real world, by monitoring differences between the simulator and real data. This allows the emulator to characterise where the simulation can be relied on, i.e.¬†we can validate the simulator.</p>
<p>Similarly, the emulator can adjudicate between simulations. This is known as <em>multi-fidelity emulation</em>. The emulator characterizes which emulations perform well where.</p>
<p>If all this modelling is done with judiscious handling of the uncertainty, the <em>computational doubt</em>, then the emulator can assist in desciding what experiment should be run next to aid a decision: should we run a simulator, in which case which one, or should we attempt to acquire data from a real world intervention.</p>
<h1 id="emukit-playground">Emukit Playground</h1>
<h2 id="emukit-playground-1">Emukit Playground</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_uq/includes/emukit-playground.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_uq/includes/emukit-playground.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<p>Emukit playground is a software toolkit for exploring the use of statistical emulation as a tool. It was built by <a href="https://twitter.com/_AdamHirst">Adam Hirst</a>, during his software engineering internship at Amazon and supervised by Cliff McCollum.</p>
<div class="figure">
<div id="emukit-playground-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/uq/emukit-playground.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="emukit-playground-magnify" class="magnify" onclick="magnifyFigure(&#39;emukit-playground&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="emukit-playground-caption" class="caption-frame">
<p>Figure: Emukit playground is a tutorial for understanding the simulation/emulation relationship. <a href="https://amzn.github.io/emukit-playground/" class="uri">https://amzn.github.io/emukit-playground/</a></p>
</div>
</div>
<div class="figure">
<div id="emukit-playground-bayes-opt-figure" class="figure-frame">
<div class="centered" style="">
<img class="negate" src="../slides/diagrams/uq/emukit-playground-bayes-opt.png" width="80%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="emukit-playground-bayes-opt-magnify" class="magnify" onclick="magnifyFigure(&#39;emukit-playground-bayes-opt&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="emukit-playground-bayes-opt-caption" class="caption-frame">
<p>Figure: Tutorial on Bayesian optimization of the number of taxis deployed from Emukit playground. <a href="https://amzn.github.io/emukit-playground/#!/learn/bayesian_optimization" class="uri">https://amzn.github.io/emukit-playground/#!/learn/bayesian_optimization</a></p>
</div>
</div>
<p>You can explore Bayesian optimization of a taxi simulation.</p>
<h2 id="gpy-a-gaussian-process-framework-in-python">GPy: A Gaussian Process Framework in Python</h2>
<p><span style="text-align:right"><span class="editsection-bracket" style="">[</span><span class="editsection" style=""><a href="https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gpy-software.md" target="_blank" onclick="ga('send', 'event', 'Edit Page', 'Edit', 'https://github.com/lawrennd/talks/edit/gh-pages/_gp/includes/gpy-software.md', 13);">edit</a></span><span class="editsection-bracket" style="">]</span></span></p>
<div class="figure">
<div id="gpy-software-figure" class="figure-frame">
<div class="centered" style="">
<img class="" src="../slides/diagrams/gp/gpy.png" width="70%" height="auto" align="center" style="background:none; border:none; box-shadow:none; display:block; margin-left:auto; margin-right:auto;vertical-align:middle">
</div>
</div>
<div id="gpy-software-magnify" class="magnify" onclick="magnifyFigure(&#39;gpy-software&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="gpy-software-caption" class="caption-frame">
<p>Figure: GPy is a BSD licensed software code base for implementing Gaussian process models in Python. It is designed for teaching and modelling. We welcome contributions which can be made through the Github repository <a href="https://github.com/SheffieldML/GPy" class="uri">https://github.com/SheffieldML/GPy</a></p>
</div>
</div>
<p>GPy is a BSD licensed software code base for implementing Gaussian process models in python. This allows GPs to be combined with a wide variety of software libraries.</p>
<p>The software itself is available on <a href="https://github.com/SheffieldML/GPy">GitHub</a> and the team welcomes contributions.</p>
<p>The aim for GPy is to be a probabilistic-style programming language, i.e.¬†you specify the model rather than the algorithm. As well as a large range of covariance functions the software allows for non-Gaussian likelihoods, multivariate outputs, dimensionality reduction and approximations for larger data sets.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> urllib.request</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>urllib.request.urlretrieve(<span class="st">&#39;https://raw.githubusercontent.com/lawrennd/talks/gh-pages/mlai.py&#39;</span>,<span class="st">&#39;mlai.py&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>urllib.request.urlretrieve(<span class="st">&#39;https://raw.githubusercontent.com/lawrennd/talks/gh-pages/teaching_plots.py&#39;</span>,<span class="st">&#39;teaching_plots.py&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>urllib.request.urlretrieve(<span class="st">&#39;https://raw.githubusercontent.com/lawrennd/talks/gh-pages/gp_tutorial.py&#39;</span>,<span class="st">&#39;gp_tutorial.py&#39;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="op">%</span>pip install gpy</span></code></pre></div>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="op">%</span>pip install pyDOE</span></code></pre></div>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="op">%</span>pip install emukit</span></code></pre></div>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="im">import</span> mlai</span>
<span id="cb8-2"><a href="#cb8-2"></a><span class="im">import</span> teaching_plots <span class="im">as</span> plot</span></code></pre></div>
<p>Related publications and links will appear here.</p>
<p>Examle paper: <span class="citation" data-cites="McKay-selecting79">McKay, Beckman, and Conover (1979)</span> <span class="citation" data-cites="Kennedy-bayesian01">Kennedy and O‚ÄôHagan (2001)</span></p>
<p>The MUCM project <a href="http://www.mucm.ac.uk/" class="uri">http://www.mucm.ac.uk/</a></p>
<p><span class="math inline"></span></p>
<p>http://www.mucm.ac.uk/Pages/Dissemination/TechnicalReports.html</p>
<blockquote>
<p>Random Sampling. Let the input values <span class="math inline">$x_1, \dots, x_\numData$</span> be a random sample from <span class="math inline"><em>f</em>(<em>x</em>)</span>. This method of sampling is perhaps the most obvious, and an entire body of statistical literature may be used in making infer- ences regarding the distribution of <span class="math inline"><em>Y</em>(<em>t</em>)</span>. Stratified Sampling. Using stratified sampling, all areas of the sample space of <span class="math inline"><em>X</em></span> are represented by input values. Let the sample space <span class="math inline"><em>S</em></span> of <span class="math inline"><em>X</em></span> be partitioned into <span class="math inline"><em>I</em></span> disjoint strata <span class="math inline"><em>S</em><sub><em>t</em></sub></span>. Let <span class="math inline"><em>œÄ</em>‚ÄÑ=‚ÄÑ<em>P</em>(<em>X</em><em>C</em><em>S</em><sub><em>i</em></sub>)</span> represent the size of <span class="math inline"><em>S</em><sub><em>i</em></sub></span>. Obtain a random sample <span class="math inline"><em>X</em><em>i</em><em>J</em>,‚ÄÜ<em>j</em>‚ÄÑ=‚ÄÑ1,‚ÄÜ‚Ä¶,‚ÄÜ<em>n</em></span> from <span class="math inline"><em>S</em><sub><em>i</em></sub></span>. Then of course the <span class="math inline"><em>n</em><sub><em>i</em></sub></span> sum to <span class="math inline"><em>N</em></span>. If <span class="math inline"><em>I</em>‚ÄÑ=‚ÄÑ1</span>, we have random sampling over the entire sample space. Latin Hypercube Sampling. The same reasoning that led to stratified sampling, ensuring that all por- tions of <span class="math inline"><em>S</em></span> were sampled, could lead further. If we wish to ensure also that each of the input variables <span class="math inline"><em>X</em><sub><em>k</em></sub></span> has all portions of its distribution represented by input values, we can divide the range of each <span class="math inline"><em>X</em><sub><em>k</em></sub></span> into <span class="math inline"><em>N</em></span> strata of equal marginal probability <span class="math inline">1/<em>N</em></span>, and sample once from each stratum. Let this sample be <span class="math inline"><em>X</em><em>k</em><em>j</em>,‚ÄÜ<em>j</em>‚ÄÑ=‚ÄÑ1,‚ÄÜ‚Ä¶,‚ÄÜ<em>N</em></span>. These form the <span class="math inline"><em>X</em><sub><em>k</em></sub></span> component, <span class="math inline"><em>k</em>‚ÄÑ=‚ÄÑ1,‚ÄÜ*,<em>K</em>,‚ÄÜ<em>i</em><em>n</em><em>X</em><em>i</em>,‚ÄÜ<em>i</em>‚ÄÑ=‚ÄÑ1,‚ÄÜ*,<em>N</em></span>. The components of the various <span class="math inline"><em>X</em>,‚ÄÜ<em>A</em>‚Ä≤<em>s</em></span> are matched at random. This method of selecting input values is an extension of quota sam- pling [13], and can be viewed as a K-dimensional extension of Latin square sampling [11]. One advantage of the Latin hypercube sample ap- pears when the output <span class="math inline"><em>Y</em>(<em>t</em>)</span> is dominated by only a few of the components of <span class="math inline"><em>X</em></span>. This method ensures that each of those components is represented in a fully stratified manner, no matter which components might turn out to be important. We mention here that the <span class="math inline"><em>N</em></span> intervals on the range of each component of X combine to form <span class="math inline"><em>N</em><em>K</em></span> cells which cover the sample space of <span class="math inline"><em>X</em></span>. These cells, which are labeled by coordinates corresponding to the inter- vals, are used when finding the properties of the sampling plan.</p>
</blockquote>
<p>This introduction is based on <a href="https://github.com/EmuKit/emukit/blob/master/notebooks/Emukit-tutorial-experimental-design-introduction.ipynb">An Introduction to Experimental Design with Emukit</a> written by Andrei Paleyes and Maren Mahsereci.</p>
<p>Experimental design.</p>
<p>Latin hypercube</p>
<p>Linear example</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-2"><a href="#cb9-2"></a></span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="im">from</span> emukit.test_functions <span class="im">import</span> forrester_function</span>
<span id="cb9-4"><a href="#cb9-4"></a><span class="im">from</span> emukit.core.loop.user_function <span class="im">import</span> UserFunctionWrapper</span>
<span id="cb9-5"><a href="#cb9-5"></a><span class="im">from</span> emukit.core <span class="im">import</span> ContinuousParameter, ParameterSpace</span></code></pre></div>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>target_function, space <span class="op">=</span> forrester_function()</span></code></pre></div>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>x_plot <span class="op">=</span> np.linspace(space.parameters[<span class="dv">0</span>].<span class="bu">min</span>, space.parameters[<span class="dv">0</span>].<span class="bu">max</span>, <span class="dv">301</span>)[:, <span class="va">None</span>]</span>
<span id="cb11-2"><a href="#cb11-2"></a>y_plot <span class="op">=</span> target_function(x_plot)</span></code></pre></div>
<div class="figure">
<div id="forrester-function-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/forrester-function.svg" width="80%" style=" ">
</object>
</div>
<div id="forrester-function-magnify" class="magnify" onclick="magnifyFigure(&#39;forrester-function&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="forrester-function-caption" class="caption-frame">
<p>Figure: The Forrester function <span class="citation" data-cites="Forrester-engineering08">(Forrester, S√≥bester, and Keane 2008)</span>.</p>
</div>
</div>
<h2 id="initial-design">Initial Design</h2>
<p>Usually, before we start the actual ExpDesign loop we need to gather a few observations such that we can fit the model. This is called the initial design and common strategies are either a predefined grid or sampling points uniformly at random.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a>X_init <span class="op">=</span> np.array([[<span class="fl">0.2</span>],[<span class="fl">0.6</span>], [<span class="fl">0.9</span>]])</span>
<span id="cb12-2"><a href="#cb12-2"></a>Y_init <span class="op">=</span> target_function(X_init)</span></code></pre></div>
<div class="figure">
<div id="forrester-function-initial-design-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/forrester-function-initial-design.svg" width="80%" style=" ">
</object>
</div>
<div id="forrester-function-initial-design-magnify" class="magnify" onclick="magnifyFigure(&#39;forrester-function-initial-design&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="forrester-function-initial-design-caption" class="caption-frame">
<p>Figure: The initial design for the Forrester function example.</p>
</div>
</div>
<h2 id="the-model">The Model</h2>
<p>Now we can start with the ExpDesign loop by first fitting a model on the collected data. A popular model for ExpDesign is a Gaussian process (GP) which defines a probability distribution across classes of functions, typically smooth, such that each linear finite-dimensional restriction is multivariate Gaussian <span class="citation" data-cites="Rasmussen:book06">(Rasmussen and Williams 2006)</span>. Gaussian processes are fully parametrized by a mean <span class="math inline">$\mu(\inputVector)$</span> and a covariance function <span class="math inline">$\kernelScalar(\inputVector,\inputVector^\prime)$</span>. Without loss of generality <span class="math inline">$\mu(\inputVector)$</span> is assumed to be zero. The covariance function <span class="math inline">$\kernelScalar(\inputVector,\inputVector^\prime)$</span> characterizes the smoothness and other properties of <span class="math inline">$\mappingFunction$</span>. It is known that the kernel of the process has to be continuous, symmetric and positive definite. A widely used kernel is the exponentiated quadratic or RBF kernel: <br /><span class="math display">$$ 
\kernelScalar(\inputVector,\inputVector^\prime) = \alpha \exp{ \left(-\frac{\|\inputVector-\inputVector^\prime\|^2}{\ell}\right)} 
$$</span><br /> where <span class="math inline"><em>Œ±</em></span> and <span class="math inline">‚Ñì</span> are hyperparameters.</p>
<p>To denote that <span class="math inline">$\mappingFunction$</span> is a sample from a GP with mean <span class="math inline"><em>Œº</em></span> and covariance <span class="math inline"><em>k</em></span> we write <br /><span class="math display">$$
\mappingFunction \sim \mathcal{GP}(\mu,k).
$$</span><br /></p>
<p>For regression tasks, the most important feature of GPs is that process priors are conjugate to the likelihood from finitely many observations <span class="math inline">$\dataMatrix = (y_1,\dots,y_\numData)^\top$</span> and <span class="math inline">$\inputMatrix =\{\inputVector_1,\dots,\inputVector_\numData\}$</span>, <span class="math inline">$\inputVector_i\in \mathcal{X}$</span> of the form <span class="math inline">$\dataScalar_i = \mappingFunction(\inputVector_i) + \noiseScalar_i$</span> where <span class="math inline">$\noiseScalar_i \sim \gaussianSamp{0}{\dataStd^2}$</span> and we typically estimate <span class="math inline">$\dataStd^2$</span> by maximum likelihood alongside <span class="math inline"><em>Œ±</em></span> and <span class="math inline">‚Ñì</span>.</p>
<p>We obtain the Gaussian posterior <br /><span class="math display">$$
\mappingFunction(\inputVector^*)|\inputMatrix, \dataMatrix, \theta \sim \gaussianSamp{\mu(\inputVector^*)}{\sigma^2(\inputVector^*)},
$$</span><br /> where <span class="math inline">$\mu(\inputVector^*)$</span> and <span class="math inline">$\sigma^2(\inputVector^*)$</span> have a closed form solution as we‚Äôve seen in the earlier lectures (see also <span class="citation" data-cites="Rasmussen:book06">Rasmussen and Williams (2006)</span>).</p>
<p>Note that Gaussian processes are also characterized by hyperparameters, for example in the exponatiated quadratic case we have <span class="math inline">$\paramVector = \left\{ \alpha, \ell, \dataStd^2 \right\}$</span> for the scale of the covariance, the lengthscale and the noise variance. Here, for simplicitly we will keep these hyperparameters fixed. However, we will usually either optimize or sample these hyperparameters using the marginal loglikelihood of the GP.</p>
<p>In this module we‚Äôve focussed on Gaussian processes, but we could also use any other model that returns a mean <span class="math inline">$\mu(\inputVector)$</span> and variance <span class="math inline">$\sigma^2(\inputVector)$</span> on an arbitrary input points <span class="math inline">$\inputVector$</span> such as Bayesian neural networks or random forests. In Emukit these different models can be used by defining a new <code>ModelWrapper</code>.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="im">import</span> GPy</span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="im">from</span> emukit.model_wrappers.gpy_model_wrappers <span class="im">import</span> GPyModelWrapper</span></code></pre></div>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a>kern <span class="op">=</span> GPy.kern.RBF(<span class="dv">1</span>, lengthscale<span class="op">=</span><span class="fl">0.08</span>, variance<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb14-2"><a href="#cb14-2"></a>gpy_model <span class="op">=</span> GPy.models.GPRegression(X_init, Y_init, kern, noise_var<span class="op">=</span><span class="fl">1e-10</span>)</span>
<span id="cb14-3"><a href="#cb14-3"></a>emukit_model <span class="op">=</span> GPyModelWrapper(gpy_model)</span>
<span id="cb14-4"><a href="#cb14-4"></a></span>
<span id="cb14-5"><a href="#cb14-5"></a>mu_plot, var_plot <span class="op">=</span> emukit_model.predict(x_plot)</span></code></pre></div>
<div class="figure">
<div id="forrester-function-entropy-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/forrester-function-entropy.svg" width="80%" style=" ">
</object>
</div>
<div id="forrester-function-entropy-magnify" class="magnify" onclick="magnifyFigure(&#39;forrester-function-entropy&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="forrester-function-entropy-caption" class="caption-frame">
<p>Figure: The emulator fitted to the Forrester function with only three observations. The error bars show 1, 2 and 3 standard deviations.</p>
</div>
</div>
<h2 id="the-acquisition-function">The Acquisition Function</h2>
<p>In the second step of our ExpDesign loop we use our model to compute the acquisition function. We‚Äôll review two different forms of acquisition funciton for doing this.</p>
<h3 id="uncertainty-sampling">Uncertainty Sampling</h3>
<p>In uncertainty sampling (US) we hoose the next value <span class="math inline">$\inputVector_{n+1}$</span> at the location where the model on <span class="math inline">$\mappingFunction(\inputVector)$</span> has the highest marginal predictive variance <br /><span class="math display">$$
a_{US}(\inputVector) = \sigma^2(\inputVector).
$$</span><br /> This makes sure, that we learn the function <span class="math inline">$\mappingFunction(\cdot)$</span> everywhere on <span class="math inline">ùïè</span> to a similar level of absolute error.</p>
<h3 id="integrated-variance-reduction">Integrated Variance Reduction</h3>
<p>In the integrated variance reduction (IVR) you choose the next value <span class="math inline">$\inputVector_{n+1}$</span> such that the total variance of the model is reduced maximally <span class="citation" data-cites="Sacks-design89">(Sacks et al. 1989)</span>, <br /><span class="math display">$$
a_{IVR} = \int_{\mathbb{X}}[\sigma^2(\inputVector') - \sigma^2(\inputVector'; \inputVector)]\text{d}\inputVector'\approx 
\frac{1}{\# \text{samples}}\sum_i^{\# \text{samples}}[\sigma^2(\inputVector_i) - \sigma^2(\inputVector_i; \inputVector)].
$$</span><br /> Here <span class="math inline">$\sigma^2(\inputVector'; \inputVector)$</span> is the predictive variance at <span class="math inline">$\inputVector'$</span> had <span class="math inline">$\inputVector$</span> been observed. Thus IVR computes the overall reduction in variance (for all points in <span class="math inline">ùïè</span>) had <span class="math inline"><em>f</em></span> been observed at <span class="math inline">$\inputVector$</span>.</p>
<p>The finite sum approximation on the right hand side of the equation is usually used because the integral over <span class="math inline">$\inputVector'$</span> is not analytic. In that case <span class="math inline">$\inputVector_i$</span> are sampled randomly. For a GP model the right hand side simplifies to <br /><span class="math display">$$
a_{LCB} \approx \frac{1}{\# \text{samples}}\sum_i^{\# \text{samples}}\frac{\kernelScalar^2(\inputVector_i, \inputVector)}{\sigma^2(\inputVector)}.
$$</span><br /></p>
<p>IVR is arguably the more principled approach, but often US is preferred over IVR simply because it lends itself to gradient based optimization more easily, is cheaper to compute, and is exact.</p>
<p>For both of them (stochastic) gradient base optimizers are used to retrieve <span class="math inline">$\inputVector_{n+1} \in \operatorname*{arg\:max}_{\inputVector \in \mathbb{X}} a(\inputVector)$</span>.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="im">from</span> emukit.experimental_design.acquisitions <span class="im">import</span> IntegratedVarianceReduction, ModelVariance</span></code></pre></div>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a>us_acquisition <span class="op">=</span> ModelVariance(emukit_model)</span>
<span id="cb16-2"><a href="#cb16-2"></a>ivr_acquisition <span class="op">=</span> IntegratedVarianceReduction(emukit_model, space)</span>
<span id="cb16-3"><a href="#cb16-3"></a></span>
<span id="cb16-4"><a href="#cb16-4"></a>us_plot <span class="op">=</span> us_acquisition.evaluate(x_plot)</span>
<span id="cb16-5"><a href="#cb16-5"></a>ivr_plot <span class="op">=</span> ivr_acquisition.evaluate(x_plot)</span></code></pre></div>
<div class="figure">
<div id="experimental-design-acquisition-functions-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/experimental-design-acquisition-functions-forrester.svg" width="80%" style=" ">
</object>
</div>
<div id="experimental-design-acquisition-functions-magnify" class="magnify" onclick="magnifyFigure(&#39;experimental-design-acquisition-functions&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="experimental-design-acquisition-functions-caption" class="caption-frame">
<p>Figure: The <em>uncertainty sampling</em> and <em>integrated variance reduction</em> acquisition functions for the Forrester example.</p>
</div>
</div>
<h2 id="evaluating-the-objective-function">Evaluating the objective function</h2>
<p>To find the next point to evaluate we optimize the acquisition function using a standard gradient descent optimizer.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a><span class="im">from</span> emukit.core.optimization <span class="im">import</span> GradientAcquisitionOptimizer</span></code></pre></div>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a>optimizer <span class="op">=</span> GradientAcquisitionOptimizer(space)</span>
<span id="cb18-2"><a href="#cb18-2"></a>x_new, _ <span class="op">=</span> optimizer.optimize(us_acquisition)</span></code></pre></div>
<div class="figure">
<div id="experimental-design-acquisition-functions-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/experimental-design-acquisition-next-point-forrester.svg" width="80%" style=" ">
</object>
</div>
<div id="experimental-design-acquisition-functions-magnify" class="magnify" onclick="magnifyFigure(&#39;experimental-design-acquisition-functions&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="experimental-design-acquisition-functions-caption" class="caption-frame">
<p>Figure: The maxima of the acquisition function is found and this point is selected for inclusion.</p>
</div>
</div>
<p>Afterwards we evaluate the true objective function and append it to our initial observations.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a>y_new <span class="op">=</span> target_function(x_new)</span></code></pre></div>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a>X <span class="op">=</span> np.append(X_init, x_new, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb20-2"><a href="#cb20-2"></a>Y <span class="op">=</span> np.append(Y_init, y_new, axis<span class="op">=</span><span class="dv">0</span>)</span></code></pre></div>
<p>After updating the model, you can see that the uncertainty about the true objective function in this region decreases and our model becomes more certain.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a>emukit_model.set_data(X, Y)</span>
<span id="cb21-2"><a href="#cb21-2"></a>mu_plot, var_plot <span class="op">=</span> emukit_model.predict(x_plot)</span></code></pre></div>
<div class="figure">
<div id="forrester-function-multi-errorbars-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/forrester-function-multi-errorbars.svg" width="80%" style=" ">
</object>
</div>
<div id="forrester-function-multi-errorbars-magnify" class="magnify" onclick="magnifyFigure(&#39;forrester-function-multi-errorbars&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="forrester-function-multi-errorbars-caption" class="caption-frame">
<p>Figure: The target Forrester function plotted alongside the emulation model and error bars from the emulation at 1, 2 and 3 standard deviations.</p>
</div>
</div>
<p>Entropy of posterior</p>
<h2 id="emukits-experimental-design-interface">Emukit‚Äôs experimental design interface</h2>
<p>Of course in practice we don‚Äôt want to implement all of these steps our self. Emukit provides a convenient and flexible interface to apply experimental design. Below we can see how to run experimental design on the exact same function for 10 iterations.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a><span class="im">from</span> emukit.experimental_design.experimental_design_loop <span class="im">import</span> ExperimentalDesignLoop</span></code></pre></div>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a>ed <span class="op">=</span> ExperimentalDesignLoop(space<span class="op">=</span>space, model<span class="op">=</span>emukit_model)</span>
<span id="cb23-2"><a href="#cb23-2"></a></span>
<span id="cb23-3"><a href="#cb23-3"></a>ed.run_loop(target_function, <span class="dv">10</span>)</span></code></pre></div>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a>mu_plot, var_plot <span class="op">=</span> ed.model.predict(x_plot)</span></code></pre></div>
<div class="figure">
<div id="forrester-function-full-fit-figure" class="figure-frame">
<object class="svgplot " data="../slides/diagrams/uq/forrester-function-full-fit.svg" width="80%" style=" ">
</object>
</div>
<div id="forrester-function-full-fit-magnify" class="magnify" onclick="magnifyFigure(&#39;forrester-function-full-fit&#39;)">
<img class="img-button" src="{{ '/assets/images/Magnify_Large.svg' | relative_url }}" style="width:1.5ex">
</div>
<div id="forrester-function-full-fit-caption" class="caption-frame">
<p>Figure: The fit of the model to the Forrester function.</p>
</div>
</div>
<h2 id="thanks">Thanks!</h2>
<p>For more information on these subjects and more you might want to check the following resources.</p>
<ul>
<li>twitter: <a href="https://twitter.com/lawrennd">@lawrennd</a></li>
<li>podcast: <a href="http://thetalkingmachines.com">The Talking Machines</a></li>
<li>newspaper: <a href="http://www.theguardian.com/profile/neil-lawrence">Guardian Profile Page</a></li>
<li>blog: <a href="http://inverseprobability.com/blog.html">http://inverseprobability.com</a></li>
</ul>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Forrester-engineering08">
<p>Forrester, Alexander I. J., Andr√°s S√≥bester, and Andy J. Keane. 2008. <em>Engineering Design via Surrogate Modelling: A Practical Guide</em>. wiley. <a href="https://doi.org/10.1002/9780470770801">https://doi.org/10.1002/9780470770801</a>.</p>
</div>
<div id="ref-Kennedy-bayesian01">
<p>Kennedy, Marc C., and Anthony O‚ÄôHagan. 2001. ‚ÄúBayesian Calibration of Computer Models.‚Äù <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 63 (3): 425‚Äì64. <a href="https://doi.org/10.1111/1467-9868.00294">https://doi.org/10.1111/1467-9868.00294</a>.</p>
</div>
<div id="ref-McKay-selecting79">
<p>McKay, Michael D., Richard J. Beckman, and W. Jay Conover. 1979. ‚ÄúA Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code.‚Äù <em>Technometrics</em> 21 (2): 239‚Äì45. <a href="http://www.jstor.org/stable/1268522">http://www.jstor.org/stable/1268522</a>.</p>
</div>
<div id="ref-Rasmussen:book06">
<p>Rasmussen, Carl Edward, and Christopher K. I. Williams. 2006. <em>Gaussian Processes for Machine Learning</em>. Cambridge, MA: mit.</p>
</div>
<div id="ref-Sacks-design89">
<p>Sacks, Jerome, William J. Welch, Toby J. Mitchell, and Henry P. Wynn. 1989. ‚ÄúDesign and Analysis of Computer Experiments.‚Äù <em>Statistical Science</em> 4 (4): 409‚Äì23. <a href="https://doi.org/10.1214/ss/1177012413">https://doi.org/10.1214/ss/1177012413</a>.</p>
</div>
</div>

